bb-server
* features (implemented)
** cookieless domains
   http://www.ravelrumba.com/blog/static-cookieless-domain/
** logging   
*** logrotation:
    You can use logrotate which is included in most Linux distributions and is
    used for rotating system log files, as well as used by other software like
    Apache.

Add a file to /etc/logrotate.d/

/path/to/server.log {
  daily         # how often to rotate
  rotate 10     # max num of log files to keep
  missingok     # don't panic if the log file doesn't exist
  notifempty    # ignore empty files
  compress      # compress rotated log file with gzip
  sharedscripts # no idea what it does, but it's in all examples
  copytruncate  # needed for forever to work properly
  dateext       # adds date to filename 
  dateformat %Y-%m-%d.
}
http://www.thegeekstuff.com/2010/07/logrotate-examples/
    
** set server root
** set custom port and address
  or auto find first available port from 8080
  server by default from ./public, otherwise ./
** send dir contents
** auto index
** favicon
Don't have a favicon.ico in the rootfolder, but add it to the server
options so you can easily control cache header expire

** pluggable GET and POST handlers
  format is different from v3.x!!!! 

** make the site crawlable
   Which means serve static pages generated by a headless browser. ok
then..
https://developers.google.com/webmasters/ajax-crawling/docs/specification
https://developers.google.com/webmasters/ajax-crawling/docs/html-snapshot
From wikipedia:
The leading search engines, such as Google, Bing and Yahoo!, use
crawlers to find pages for their algorithmic search results. Pages
that are linked from other search engine indexed pages do not need to
be submitted because they are found automatically. Some search
engines, notably Yahoo!, operate a paid submission service that
guarantee crawling for either a set fee or cost per click.[30] Such
programs usually guarantee inclusion in the database, but do not
guarantee specific ranking within the search results.[31] Two major
directories, the Yahoo Directory and the Open Directory Project both
require manual submission and human editorial review.[32] Google
offers Google Webmaster Tools, for which an XML Sitemap feed can be
created and submitted for free to ensure that all pages are found,
especially pages that are not discoverable by automatically following
links.[33]
http://www.yearofmoo.com/2012/11/angularjs-and-seo.html
https://github.com/deanmao/node-chimera
https://github.com/steeve/angular-seo
http://www.marketingpilgrim.com/7-minute-seo-guide
http://static.googleusercontent.com/external_content/untrusted_dlcp/www.google.com/en//webmasters/docs/search-engine-optimization-starter-guide.pdf
*** cdn
*** send real 404's
      http://prerender.io/server
      
*** Recognizing bots: 
**** Client asks or escaped fragment:
  https://developers.google.com/webmasters/ajax-crawling/docs/specification
  https://developers.google.com/webmasters/ajax-crawling/docs/getting-started
**** regex the user string:
  robot|spider|crawler|curl|slurp or:
  bot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo!
  slurp|mediapartners-google
  or:
  http://stackoverflow.com/questions/544450/detecting-honest-web-crawlers
**** or consult a database:
  weekly updated cached parser 
   https://github.com/GUI/uas-parser
   https://npmjs.org/package/uas-parser
   
   or use the standard one, but no bot data:
   https://npmjs.org/package/useragent
   
*** serve static version:    
     https://github.com/moviepilot/seoserver
     Installs phantomjs as a node module, uses exec
     https://github.com/Obvious/phantomjs
     Phantom api
https://github.com/ariya/phantomjs/wiki/API-Reference-WebPage#wiki-webpage-plainText
More info on phantom:
http://thedigitalself.com/blog/seo-and-javascript-with-phantomjs-server-side-rendering
http://www.sitepoint.com/headless-webkit-and-phantomjs/
*** More    
  identifying googlebot by ip:
  by reverse dns lookup of ip, has to be in googlebot.com domain
  then forward dns lookup, which should get you your ip back again.
  https://support.google.com/webmasters/answer/80553?hl=en
  How to identify search engine spiders and webbots
  http://www.jafsoft.com/searchengines/spider_hunting.html
   http://stackoverflow.com/questions/544450/detecting-honest-web-crawlers
*** implementation   
   https://github.com/icodeforlove/node-express-renderer
   https://github.com/steeve/angular-seo
   http://www.yearofmoo.com/2012/11/angularjs-and-seo.html
   https://github.com/markselby/node-angular-seo
   https://github.com/bfirsh/otter
   http://backbonetutorials.com/seo-for-single-page-apps/
   https://github.com/apiengine/seoserver
   
** recast   
*** images
    http://tinypng.org/
    http://www.smushit.com/ysmush.it/
    https://kraken.io/web-interface
   drop images onto the tool and they get compressed 
   http://trimage.org/ 
   
   convert from jpg to png:
   mogrify -format jpg *.png  
   shell script:
   for img in *.png; do
    filename=${img%.*}
    convert "$filename.png" "$filename.jpg"
done
http://superuser.com/questions/71028/batch-converting-png-to-jpg-in-linux
   resizing 
Install imagemagick then
mogrify -resize x450 *.jpg
to resize all images in dir
and:
mogrify -quality 80 *.jpg
to compress
   jpegtran is in libjpeg-turbo-progs 
   pngcrush
  optipng 
  pngquant
  pngout
   Proxy them 
 https://github.com/discore/iproxy
Resize them:
  https://npmjs.org/package/grunt-image-resize
  https://npmjs.org/package/image-shrink
 Optimize:
 https://npmjs.org/package/imageoptmizer-brunch
 https://npmjs.org/package/imagemin
 https://npmjs.org/package/grunt-pngmin
 
*** transpile 
*** minify and gzip!
**** threshold for gzipping?
**** uglify 
**** minify css and html 
Maybe also minify css and html?
https://github.com/kangax/html-minifier
Top of npm list:
https://github.com/GoalSmashers/clean-css
Lot of docs:
http://bem.info/tools/csso/install/
Port of yui compressor
https://github.com/fmarcia/UglifyCSS

** caching
*** memory and disk cache
    
** modified-since
   
** cache busting  
   
Hardcoded requests for static resources should be stamped with the
resources last modified data (in htm-builder). When the resource
changes, the site will have to be rebuilt, but it will garantue that
the requests for files that changeed and only them will get through to
bb-server

Dynamic requests for static resources from client can also be
timestamped, they will be sent out with max-age > 0, this only makes
sens when the stamp is tied to another file that gives them a fixed
stamp to attach, for example in index.html;

   
carry over mtime to cache.js
hash to store cache items should of stamp -AND- key!!
** spa 
always send index.html when requesting non-file
when serving spa and you don't want to use #! you always serve
index.html and then let the app sort out the routing.
http://docs.angularjs.org/guide/dev_guide.services.$location
   
** add sitemap to firstdoor site
** put up firstdoor.axion5.com and use cloudflare
  update npm and git: bb-server url_washer cachejs recaster phantom-sitemap
  sync bb4 branch of firstdoor to linode
  add forever task

  
  
** sessions and authentication
*** send cache-control private when using sessions!!
*** sign in with
    google, facebook, linkedin, github, persona, twitter, basic
    to start of with, incorporate persona into server
    
** set cache control=private when using sessions!!
    
** websocket and https server
** forwarder: test and clean up!!
     I put it in a module, but is not tested yet
** security!!! 
   http://www.adambarth.com/papers/2008/barth-jackson-mitchell-b.pdf
   http://shiflett.org/articles/session-hijacking
   https://developer.mozilla.org/en-US/docs/Mozilla/Persona/Security_Considerations?redirectlocale=en-US&redirectslug=Persona%2FSecurity_Considerations
   
*****  Implement CSRF protection

In a CSRF (Cross-Site Request Forgery) login attack, an attacker uses
a cross-site request forgery to log the user into a web site using the
attacker's credentials.

For example: a user visits a malicious web site containing a form
element. The form's action attribute is set to an HTTP POST request to
http://www.google.com/login, supplying the attacker's username and
password. When the user submits the form, the request is sent to
Google, the login succeeds and the Google server sets a cookie in the
user's browser. Now the user's unknowingly logged into the attacker's
Google account.

The attack can be used to gather sensitive information about the
user. For example, Google's Web History feature logs all the user's
Google search terms. If a user is logged into the attacker's Google
account and the attacker has Web History enabled, then the user is
giving the attacker all this information.

CSRF login attacks, and potential defenses against them, are
documented more fully in Robust Defenses for Cross-Site Request
Forgery (PDF). They're not specific to Persona: most login mechanisms
are potentially vulnerable to them.

There are a variety of techniques which can be used to protect a site
from CSRF login attacks, which are documented more fully in the study
above.

One approach is to create a secret identifier in the server, shared
with the browser, and require the browser to supply it when making
login requests. For example:

As soon as the user lands on your site, before they try to log in,
create a session for them on the server. Store the session ID in a
browser cookie.  On the server, generate a random string of at least
10 alphanumeric characters. A randomly generated UUID is a good
option. This is the CSRF token. Store it in the session.  Deliver the
CSRF token to the browser by either embedding it in JavaScript or HTML
as a hidden form variable.  Ensure that the AJAX submission or form
POST includes the CSRF token.  On the server side, before accepting an
assertion, check that the submitted CSRF token matches the
session-stored CSRF token.
***** angular security 
http://docs.angularjs.org/api/ng.$http
   
***** use secure cookies:
https://github.com/jed/cookies
https://github.com/jed/keygrip
http://mahoney.eu/2012/05/23/couchdb-cookie-authentication-nodejs-nano/#.UbAdzqBCAWM
***** csrf
     look at connect middleware for implementation 
      
     
** mail functionality
** check and fix:    
***  replace markdown with marked
   Githhub Flavored Markdown and fast:
   https://npmjs.org/package/marked
   or have a general transform plugin based on mime type? 
     
   
   
* misc
//TODO: limit sending of files to certain mimetypes and/or extensions
//TODO: option to not send mimeless files found in allowable directories.
//TODO: send certain files directly, bypassing cache with certain
//cache control headings, so we can send big files etc

* more features
** streaming of all resources
** rename defaultHandler to fileHandler or staticHandler
** rewrite phantomjs script to use a server iso console.log
   http://stackoverflow.com/questions/20751999/phantomjs-interprocess-communication
   https://github.com/ariya/phantomjs/wiki/API-Reference-WebServer
   http://phantomjs.org/api/webpage/handler/on-load-finished.html
** use cdnjs.com to host js libs
** use external hosts for images
** recaster plugin ngmin
** respond to cache-control setting in request
** server reporting and monitoring
*** little stat app/page
incorporate with logger, count connections per day per ip address
or use winston or bunyan to get json output, then query it..
*** access server logs in browser?    
  https://github.com/ethanl/connect-browser-logger
  add a get handler for example /__logs and serve page with stats
  possibly only when authorized using persona for example
*** -report to console:
https://github.com/ethanl/connect-browser-logger
*** -airbrake like, so post info somewhere
    The best:
    https://www.splunkstorm.com/storm/deployment/settings/c941c94c91fe11e3a144123139097a14
    loggly: collect logs
    //run your own realtime log forwarder server:
    http://logio.org/
    
    Other:
    airbrake: error reporting https://airbrake.io/languages //paid
    newrelic: system reporting //same as longview and opensource one
    https://papertrailapp.com/ //gem won't build
    
    https://github.com/mnutt/hummingbird
    http://docs.strongloop.com/display/DOC/StrongOps
    https://github.com/lorenwest/node-monitor
** better logging
   http://www.senchalabs.org/connect/logger.html
*** use winston and its transport ipv file, also has logrotation
    https://www.google.com.au/search?q=winston+logrotate&oq=winston+logrotate&aqs=chrome..69i57j0l2.4351j0j7&sourceid=chrome&espv=2&es_sm=91&ie=UTF-8
*** or use bunyan
    https://github.com/trentm/node-bunyan
*** clean up dichotomy of log and silent    
  Should have status out and error out and server out  
** test in mswindows
** cache in couchdb?
** separate cache out from the bb-server app?
** needs authentication, sessions, cookieless domains:   
       ,api: '_api'
    //use persona to authenticate
    ,persona: true
    ,emails: ['mail@axion5.net']
    //
    //enable server api:
    ,sitemap: true
    ,html_builder: true
    ,clear_cache: true
    ,files: true
    ,stats: true

*** api/uis for server status and logs
*** __assets
app that lets authorized users play around with the file system of
websites, or possibly of the whole hard drive.
So, uploading and downloading files, rebuild site using html-builder,
downloading snapshots, uploading snapshots

Maybe even edit js and css and html and the recipe files , as I do it
locally, but using ace editor.  

For more sophisticated system you could use placeholders for static
assets and fill these buckets in some kind of manager. Maybe store these
assets in CouchDB. You get to back it up easily through replication,
and easy retrieval and role based write/read access. And you can share
the assets with other sites then as well. 
** enable cors
     https://github.com/agrueneberg/Corser
     https://github.com/troygoode/node-cors
     send a bunch of headers and respond to options method when
     enabled. Use couchdb setup as an example for settings
** serve fancy dir
http://www.senchalabs.org/connect/directory.html
with icons, json as json, html as html, js as js, possibly with
highlighting etc, show hidden files?

** send script that listens to sockets and refreshes browser
      ala livereload perhaps
      https://www.npmjs.org/package/script-injector
      https://github.com/chrisdickinson/sse-stream
      https://developer.mozilla.org/en-US/docs/Server-sent_events/Using_server-sent_events
      https://github.com/chrisdickinson/beefy/blob/master/lib/server.js
** load diff of (js files) instead of whole file?
    share js files between server and client?
    
    
** more options about sending files or not:    
*** limit sending of files to certain mimetypes and/or extensions
*** option to not send mimeless files found in allowable directories.
*** send certain files directly, bypassing cache with certain
     
    
* good to know     
** enabling https
   create certificate signing request:
  openssl req -nodes -new -newkey rsa:2048 -keyout yourdomain.com.key -out yourdomain.com.csr 
  You end up with a private key and a certificate signing request
  Then either sign it yourself:
  openssl x509 -req -days 365 -in yourdomain.com.csr -signkey yourdomain.com.key -out yourdomain.com.crt
  Or submit to comodo or other commercial certificate provider 
  http://www.tigase.org/content/creating-and-loading-server-certificate-pem-files
    
  
** enabling persona
*** enable persona 
Enabled sessions with default options if sessions is not defined.
***   Add script tags
   <script src="https://login.persona.org/include.js"></script>
   <script src="scripts/persona.js"></script> //uses angular httpRequest
   
***   signing in and out
    $scope.signout = function($event) {
        $event.preventDefault();
        console.log('Logging out');
        navigator.id.logout();
        
    };
    $scope.signin = function($event) {
        $event.preventDefault();
        console.log('Logging in');
        navigator.id.request();
    };
   
***   persona.js
function initPersona($scope, $http) {
    var currentUser = cookie.get('persona');
    if (currentUser) $scope.signedIn = currentUser;
 
    navigator.id.watch({
        loggedInUser: currentUser,
        onlogin: function(assertion) {
            
            // A user has logged in! Here you need to:
            // 1. Send the assertion to your backend for verification and to create a session.
            // 2. Update your UI.
            console.log('posting /signin');
            $http({ 
                method: 'POST',
                url: '/signin', // This is a URL on your website.
                data: {assertion: assertion} })
                .success(function(data, status, headers, config) {
                    $scope.signedIn = data.email;
                    // $scope.$apply(;
                    cookie.set('persona', data.email);
                    console.log('Posted signin. It was a success', data);
                })
                .error(function(data, status, headers, config) {
                    cookie.remove('persona');
                    navigator.id.logout();
                    console.log('Posted signin. Failure', data, status);
                    alert("Sign in failure: " + data.reason);
                });
        },
        onlogout: function() {
            // A user has logged out! Here you need to:
            // Tear down the user's session by redirecting the user or making a call to your backend.
            // Also, make sure loggedInUser will get set to null on the next page load.
            // (That's a literal JavaScript null. Not false, 0, or undefined. null.)
            console.log('posting /signout');
            $http({
                method: 'POST',
                url: '/signout'})
                .success(function(data, status, headers, config) {
                    cookie.remove('persona');
                    $scope.signedIn = false;
                    console.log('signout post success', data);
                })
                .error(function(data, status, headers, config) {
                    cookie.remove('persona');
                    navigator.id.logout();
                    $scope.signedIn = false;
                    alert("Sign out failure: " + data.reason);
                });

        }
    });
}
   
** caching  
*** cookieless domains
    proxies will not cache a response to a request that has a Cookie header!!!
    When they are cached, which they shouldn't be, there are security
    implications!! Also, browsers add the cookie to all requests, increasing
    latency for the site.  When serving cookies, set cache-control to private!!
    When serving statics, cookieless resources from a subdomain, make sure to
    redirect example.com to www.example.com so that the browser doesn't receive
    set-cookie from the global domain, and don't set cookies on the client with
    domain *.example.com, also make sure any 3rd party js doesn't set cookies
    with a global domain.
    https://developers.google.com/speed/docs/best-practices/caching?csw=1#LeverageProxyCaching
    https://developers.google.com/speed/docs/best-practices/request#ServeFromCookielessDomain
    http://webmasters.stackexchange.com/questions/1772/how-do-i-set-up-a-cookie-less-domain
    https://drupal.org/node/1896610 Drupal specific //Good one:
    http://stackoverflow.com/questions/9334393/proxy-cacheing-what-about-cookies
   
** server performance testing 			  :server:performance:report:
   http://serverbear.com/compare/dedicated
   http://loadimpact.com/load-test/pavetheway.axion5.net-c2385151073ea37a4613c972ddd55d9b
   http://wiki.dreamhost.com/Web_Server_Performance_Comparison
   https://github.com/sivel/speedtest-cli
   pip install speedtest-cli
   http://www.midwesternmac.com/blogs/jeff-geerling/2013-vps-benchmarks-linode
   script:
   dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync    
   sysbench --test=cpu --cpu-max-prime=20000 run
   sysbench --test=cpu --cpu-max-prime=20000 --num-threads=X run
   sysbench --test=memory --memory-total-size=1G run
   speedtest-cli
***    linode:
 Testing download speed........................................
Download: 114.41 Mbit/s
Testing upload speed..................................................
Upload: 58.76 Mbit/s
*** aws:
Testing from Amazon Technologies (54.243.190.112)...
Selecting best server based on ping...
Hosted by InfoRelay Online Systems (Reston, VA) [16.06 km]: 12.795 ms
Testing download speed........................................
Download: 98.17 Mbit/s
Testing upload speed..................................................
Upload: 183.38 Mbit/s
*** digitalocean
Hosted by Isset Internet Professionals (Hilversum) [49.65 km]: 19.948 ms
Testing download speed........................................
Download: 786.76 Mbit/s
Testing upload speed..................................................
Upload: 156.88 Mbit/s

*** requests per second 
      There are 86,400 seconds in a day. If you handle 1 request per second
you are handling 86,400 requests per day. If you handle 10 requests
per second you are handling 864,000 requests per day (or nearly 1
million requests a day) and if you can handle 100 requests per second
you are handling 8,640,000 (over 8 million), so 100 requests per
second is usually a good starting point. Unfortunately, usage is not
evenly distributed like this and you tend to get bursts of requests
which can impact your server.

In your case, another factor to consider is how much data you deliver
(a 1k page, a 10k page, a 100k page, etc) and how long your sql
requests take. I tend to use a service oriented architecture where the
sql requests go over the wire to a farm of services which do the
actual sql access. I use async call backs on the front end and using
this approach I have noticed a communication overhead of about 2ms per
request. The actual sql takes about 20ms and misc overhead contributes
enough for us to say a typical request will take about 25ms.

Assuming a single threaded approach where requests happen serially
(which is not the case, but just for performance analysis) this means
you can get about 40 requests per second (1,000 ms per second divided
by 25ms per request) out of a single single threaded instance. Now in
the case of tornado you can get more than one request going at a time,
so you will get more than 40 requests per second out of a single
tornado instance.

Usually the limiting factor is the page payload and front-end
bandwidth. Assuming you deliver 100k per page and you have a 10mbs
link to your website, you can deliver about 10 pages a second
(remember 100k is in bytes, mbs is in bits; there is about a 10:1
ratio here) or put another way; you can support 10 simultaneous
users.

But lets forget about front-end bandwidth concerns (assume you are on
a cloud with unlimited bandwidth) and concentrate on server
performance. Now lets say each tornado instance as described above can
deliver roughly 40-100 requests per second (which is what we see on
our small cloud machine which is a 1Ghz Athelon with about 380Mb of
memory) then you can easily increase this by using nginx to load
balance several of the front-end (we call them MVCs or on-lines)
machines in parallel. To keep this post brief I will summarize what we
currently see ...

Using 1 Front-ends and 1 service instance we get about 25 rps.
Using 2 Front-ends and 2 service instances we get about 40 rps.
Using 4 Front-ends and 16 service instances we get about 80 rps.
Using 8 Front-ends and 32 service instances we get about 125 rps.

Not what we were expecting but we are investigating what is wrong as I
speak (for example, we also have couch db running on that box, a rails
app and some other stuff). Now keep in mind when I say "Using 8 Front-
ends and 32 service instances we get about 125 rps" what I really mean
is on that single 1Ghz machine (with about 380Mb of memory) we start
one instance of nginx (to load balance both the front and back ends)
32 instances of our back end service and 8 tornados (each on its on
port obviously) so we are probably overloading our poor 1Ghz cloud
machine but like I said, we are currently investigating. Still, we get
about 100rps for a single very small machine. I know that when I did
some tests at work (the small cloud machine is for a home start-up we
are working on) on dedicated hardware I was seeing closer to
500-700rps which is about what I expected and is not bad for a single
server.
a
So to summarize; if you want to support about a million requests a day
you need to be able to handle at least 10 requests per second
sustained. This should only require one small server.

Hope that helps a bit.
- show quoted text -

** use nodemon?   
    nodemon will watch the files in the directory that nodemon was
    started, and if they change, it will automatically restart your
    node application.
  https://github.com/rem
   
** other servers:   
 https://github.com/mjijackson/mach  
 Simplicity: straightforward mapping of HTTP requests to JavaScript function calls
Streaming: request and response bodies can be streamed
Composability: middleware composes easily using promises
Robust: Promises propagate errors up the call stack, simplifying error handling

** static cache layer example (serve a number of statics from memory)://www.senchalabs.org/connect/staticCache.html

** other build tools
grunt, gulp, brunch broccoli
 http://www.solitr.com/blog/2014/02/broccoli-first-release/?utm_source=html5weekly&utm_medium=email
   
